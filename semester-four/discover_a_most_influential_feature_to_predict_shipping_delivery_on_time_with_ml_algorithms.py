# -*- coding: utf-8 -*-
"""Discover a most influential feature to predict shipping delivery on time with ML algorithms.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ak_hF6euYKeFmY0c4p-K9_Rf8fFqcf7C

## Problem Statement

### According to our dataset, which feature does affect the E-commerce Shipping Delivery on time the most?

## Initial Setup

First, install the imbalanced-learn module and xgboost module that is necessary for our data preprocessing and modelling XGBoost.
"""

pip install imbalanced-learn

pip install xgboost

# Commented out IPython magic to ensure Python compatibility.
# Importing Necessary Libraries

# For an Exploratory Data Analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For Data Preprocessing (target variable)
from imblearn.over_sampling import ADASYN
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV

# For Modelling
from sklearn.tree import DecisionTreeClassifier  # Decision Tree Classifier
import xgboost as xgb   # XGBoost
from sklearn.linear_model import LogisticRegression  # Logistic Regression
from sklearn.ensemble import RandomForestClassifier  # Random Forest Classifier
from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors (KNN) Classifier

# For Evaluations
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, f1_score, recall_score, precision_score, make_scorer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# For Feature Importance
from sklearn.inspection import permutation_importance

# %matplotlib inline

# Load data from the certain csv file, and create a DataFrame df with the data
df = pd.read_csv("ITS65704_GroupAsgn_Sec02.csv")

"""## Exploratory Data Analysis

### Initial EDA
"""

# Extract first five observations from the dataset
df.head()

# Represents, column names, number of non-null values, and data types of columns
df.info()

# Represents the statistical summaries of the dataset
df.describe()

# Represents the size of dataset matrix (rows, columns)
df.shape

# Count each value existing in "Warehouse_block"
df["Warehouse_block"].value_counts()

# Count each value existing in "Mode_of_Shipment"
df["Mode_of_Shipment"].value_counts()

# Count each value existing in "Product_importance"
df["Product_importance"].value_counts()

# Count each value existing in "Gender"
df["Gender"].value_counts()

# Describe categorical columns' statistical summaries
df.select_dtypes(include = "object").describe()

# Confirm the max count of missing values on columns
df.isna().sum().max()

# Calculates the number of each value in "Class" column
print('Delayed', round(df['Reached.on.Time_Y.N'].value_counts()[0]/len(df) * 100,2), '% of the dataset')
print('Reached on Time', round(df['Reached.on.Time_Y.N'].value_counts()[1]/len(df) * 100,2), '% of the dataset')

"""The values in target variable are imbalanced
</br> -> we will perform resampling for the variable.

### Descriptive EDA

#### Numerical Features Visualization
"""

# Extract numerical features from the DataFrame
numeric_columns = df.select_dtypes(include = ["int64", "float64"])

# Histogram for each numerical feature
for nu_column in numeric_columns:
    plt.figure(figsize=(10,6))
    sns.histplot(df[nu_column], kde=False, bins=30)
    plt.title(f'Distribution of {nu_column}')
    plt.show()

"""The numerical variables seemed heavily imbalanced."""

# Scatter Plot for each numerical feature
for nu_column in numeric_columns:
    plt.figure(figsize=(10,6))
    plt.scatter(df[nu_column], np.arange(len(df[nu_column])))
    plt.title(f'Scatter Plot of {nu_column}')
    plt.show()

# Correlation Matrix Heatmap (before encoding)
plt.figure(figsize=(12,8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""The correlation matrix seems not having a high multicollinearity, because there aren't so much high correlations among variables."""

# Seaborn Pairplot
plt.figure(figsize=(12,8))
sns.pairplot(df)
plt.show()

"""According to the "Reached.on.Time_Y.N" column plots in the pairplot, we could confirm that the variable seems to be categorical.
</br> -> we should use ***classification*** ML algorithms for prediction.
</br>    (e.g.) Logistic Regression, Decision Tree, Random Forest, etc.

#### Categorical Features Visualization
"""

# Extract categorical features from the DataFrame
categorical_columns = df.select_dtypes(include = "object")

# Bar Chart for each categorical feature
for column in categorical_columns:
    plt.figure(figsize=(10,6))
    sns.countplot(data=df, x=column, order=df[column].value_counts().index)
    plt.title(f'Distribution of {column}')
    plt.show()

"""### Data Preprocessing - Label Encoding

This is the pre-processing work to convert categorical data into numerical data.
"""

# Create an instance of LabelEncoder Class
le = LabelEncoder()

df["Warehouse_block"] = le.fit_transform(df["Warehouse_block"].fillna('Unknown'))
df["Mode_of_Shipment"] = le.fit_transform(df["Mode_of_Shipment"].fillna('Unknown'))
df["Product_importance"] = le.fit_transform(df["Product_importance"].fillna('Unknown'))
df["Gender"] = le.fit_transform(df["Gender"].fillna('Unknown'))

df

df.info()

df.describe()

# Correlation Matrix Heatmap (Based on label-encoded data)
plt.figure(figsize=(30, 30))
sns.heatmap(df.corr(), annot = True, cmap = "coolwarm", vmin = -1, vmax = 1)
plt.show()

"""According to the correlation matrix (with an encoded data), below are the features we have considered including or excluding for modeling:
</br>

[ Include ]

- **Discount_offered**: It has a notable negative correlation (-0.38) with Reached.on.Time_Y.N, which is significant.
- **Weight_in_gms**: A negative correlation of -0.27 with Reached.on.Time_Y.N is also significant.
</br>

[ Possibly Exclude ]

- **ID**: It is a feature for identifying each observation, so would be excluded.
- **Gender**: Its correlation with Reached.on.Time_Y.N is relatively low (0.0047).
- **Warehouse_Block**: Again, the correlation with the target variable is low (0.0052).
- **Prior_purchases**: The correlation is not strong enough (0.0087) to indicate a strong linear relationship.
- **Customer_care_calls**: The correlation is not very strong (0.0094).
- **Mode_of_Shipment**: Shows relatively low correlation (-0.00054) with Reached.on.Time_Y.N.
- **Product_importance**: Has a correlation of -0.023 with Reached.on.Time_Y.N, it's not as strong as some others.
- **Cost_of_the_Product**: This has a correlation of -0.074 with the target, which is not worth considering.
- **Customer_rating**: Since the correlation is low (0.013), it is not worth including.

Therefore, we will perform Data Wrangling by excluding above 9 features that are "Possibly Exclude" from the features for modeling, in next step.

## Modelling

### Split data into Features and Target, create Train and Test set
"""

# creating features and target
X = df[["Discount_offered", "Weight_in_gms"]]
y = df["Reached.on.Time_Y.N"]

"""Besides the features and target without resampling by ADASYN, we would create another features and target "X_resampled" and "y_resampled"."""

# Instantiate ADASYN
adasyn = ADASYN()

# Resample the dataset
X_resampled, y_resampled = adasyn.fit_resample(X, y)

# Splitting the data into training and testing data sets
X_sampled_train, X_sampled_test, y_sampled_train, y_sampled_test = train_test_split(X_resampled, y_resampled, test_size = 0.2, random_state = 42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# Scaling the data using StandardScaler()
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
X_sampled_train = sc.fit_transform(X_sampled_train)
X_sampled_test = sc.fit_transform(X_sampled_test)

"""### Logistic Regression

#### Without Resampling
"""

# Initializing the Logistic Regression model
LR = LogisticRegression()

# Fitting the model with training data
LR.fit(X_train, y_train)

# Making predictions on the train and test data
lr_train_preds = LR.predict(X_train)
lr_test_preds = LR.predict(X_test)

# Evaluation scores for Train set
mse = round(mean_squared_error(y_train, lr_train_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_train, lr_train_preds), 4)
r2 = round(r2_score(y_train, lr_train_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

# Evaluation scores for Test set
mse = round(mean_squared_error(y_test, lr_test_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_test, lr_test_preds), 4)
r2 = round(r2_score(y_test, lr_test_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

"""#### After ADASYN Resampling"""

# Initializing the Logistic Regression model
sampled_lr = LogisticRegression()

# Fitting the model with training data
sampled_lr.fit(X_sampled_train, y_sampled_train)

# Making predictions on the train and test data
lr_sampled_train_preds = sampled_lr.predict(X_sampled_train)
lr_sampled_test_preds = sampled_lr.predict(X_sampled_test)

# Evaluation scores for Train set
mse = round(mean_squared_error(y_sampled_train, lr_sampled_train_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_sampled_train, lr_sampled_train_preds), 4)
r2 = round(r2_score(y_sampled_train, lr_sampled_train_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

# Evaluation scores for Test set
mse = round(mean_squared_error(y_sampled_test, lr_sampled_test_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_sampled_test, lr_sampled_test_preds), 4)
r2 = round(r2_score(y_sampled_test, lr_sampled_test_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

"""### Decision Tree

#### Without Resampling
"""

# Define random seed as 42
random_state = 42

# Initializing the Decision Tree model
DT = DecisionTreeClassifier(random_state = random_state)

# Fitting the model with training data
DT.fit(X_train, y_train)

# Making predictions on the train and test data
dt_train_preds = DT.predict(X_train)
dt_test_preds = DT.predict(X_test)

# Evaluation scores for Train set
mse = round(mean_squared_error(y_train, dt_train_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_train, dt_train_preds), 4)
r2 = round(r2_score(y_train, dt_train_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

# Evaluation scores for Test set
mse = round(mean_squared_error(y_test, dt_test_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_test, dt_test_preds), 4)
r2 = round(r2_score(y_test, dt_test_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

"""#### After ADASYN Resampling"""

# Initializing the Decision Tree model
sampled_DT = DecisionTreeClassifier(random_state = random_state)

# Fitting the model with training data
sampled_DT.fit(X_sampled_train, y_sampled_train)

# Making predictions on the train and test data
dt_sampled_train_preds = sampled_DT.predict(X_sampled_train)
dt_sampled_test_preds = sampled_DT.predict(X_sampled_test)

# Evaluation scores for Train set
mse = round(mean_squared_error(y_sampled_train, dt_sampled_train_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_sampled_train, dt_sampled_train_preds), 4)
r2 = round(r2_score(y_sampled_train, dt_sampled_train_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

# Evaluation scores for Test set
mse = round(mean_squared_error(y_sampled_test, dt_sampled_test_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_sampled_test, dt_sampled_test_preds), 4)
r2 = round(r2_score(y_sampled_test, dt_sampled_test_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

"""### Random Forest

#### Without Resampling
"""

# Initializing the Random Forest model
RFC = RandomForestClassifier()

# Fitting the model with training data
RFC.fit(X_train, y_train)

# Making predictions on the train and test data
rfc_train_preds = RFC.predict(X_train)
rfc_test_preds = RFC.predict(X_test)

# Evaluation scores for Train set
mse = round(mean_squared_error(y_train, rfc_train_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_train, rfc_train_preds), 4)
r2 = round(r2_score(y_train, rfc_train_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

# Evaluation scores for Test set
mse = round(mean_squared_error(y_test, rfc_test_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_test, rfc_test_preds), 4)
r2 = round(r2_score(y_test, rfc_test_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

"""#### After ADASYN Resampling"""

# Initializing the Random Forest model
sampled_RFC = RandomForestClassifier()

# Fitting the model with training data
sampled_RFC.fit(X_sampled_train, y_sampled_train)

# Making predictions on the train and test data
rfc_sampled_train_preds = sampled_RFC.predict(X_sampled_train)
rfc_sampled_test_preds = sampled_RFC.predict(X_sampled_test)

# Evaluation scores for Train set
mse = round(mean_squared_error(y_sampled_train, rfc_sampled_train_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_sampled_train, rfc_sampled_train_preds), 4)
r2 = round(r2_score(y_sampled_train, rfc_sampled_train_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

# Evaluation scores for Test set
mse = round(mean_squared_error(y_sampled_test, rfc_sampled_test_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_sampled_test, rfc_sampled_test_preds), 4)
r2 = round(r2_score(y_sampled_test, rfc_sampled_test_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

"""### XGBoost

#### Without Resampling
"""

# Create an instance of XGBoost
xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)

# Fitting the model with training and testing data
xgb_model.fit(X_train, y_train)

# Making predictions on the train and test data
xgb_train_preds = xgb_model.predict(X_train)
xgb_test_preds = xgb_model.predict(X_test)

# Evaluation scores for Train set
mse = round(mean_squared_error(y_train, xgb_train_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_train, xgb_train_preds), 4)
r2 = round(r2_score(y_train, xgb_train_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

# Evaluation scores for Test set
mse = round(mean_squared_error(y_test, xgb_test_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_test, xgb_test_preds), 4)
r2 = round(r2_score(y_test, xgb_test_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

"""#### After ADASYN Resampling"""

# Create an instance of XGBoost
sampled_xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)

# Fitting the model with training and testing data
sampled_xgb_model.fit(X_sampled_train, y_sampled_train)

# Making predictions on the train and test data
xgb_sampled_train_preds = sampled_xgb_model.predict(X_sampled_train)
xgb_sampled_test_preds = sampled_xgb_model.predict(X_sampled_test)

# Evaluation scores for Train set
mse = round(mean_squared_error(y_sampled_train, xgb_sampled_train_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_sampled_train, xgb_sampled_train_preds), 4)
r2 = round(r2_score(y_sampled_train, xgb_sampled_train_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

# Evaluation scores for Train set
mse = round(mean_squared_error(y_sampled_test, xgb_sampled_test_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_sampled_test, xgb_sampled_test_preds), 4)
r2 = round(r2_score(y_sampled_test, xgb_sampled_test_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

"""### K-Nearest Neighbors (KNN) classifier

#### Without Resampling
"""

# Create an instance of KNeighborsClassifier()
KNC = KNeighborsClassifier()

# Fitting the model with training data
KNC.fit(X_train, y_train)

# Making predictions on the train and test data
knc_train_preds = KNC.predict(X_train)
knc_test_preds = KNC.predict(X_test)

# Evaluation scores for Train set
mse = round(mean_squared_error(y_train, knc_train_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_train, knc_train_preds), 4)
r2 = round(r2_score(y_train, knc_train_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

# Evaluation scores for Test set
mse = round(mean_squared_error(y_test, knc_test_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_test, knc_test_preds), 4)
r2 = round(r2_score(y_test, knc_test_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

"""#### After ADASYN Resampling"""

# Create an instance of KNeighborsClassifier()
sampled_KNC = KNeighborsClassifier()

# Fitting the model with training data
sampled_KNC.fit(X_sampled_train, y_sampled_train)

# Making predictions on the train and test data
knc_sampled_train_preds = sampled_KNC.predict(X_sampled_train)
knc_sampled_test_preds = sampled_KNC.predict(X_sampled_test)

# Evaluation scores for Train set
mse = round(mean_squared_error(y_sampled_train, knc_sampled_train_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_sampled_train, knc_sampled_train_preds), 4)
r2 = round(r2_score(y_sampled_train, knc_sampled_train_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

# Evaluation scores for Test set
mse = round(mean_squared_error(y_sampled_test, knc_sampled_test_preds), 4)
rmse = round(np.sqrt(mse), 4)
mae = round(mean_absolute_error(y_sampled_test, knc_sampled_test_preds), 4)
r2 = round(r2_score(y_sampled_test, knc_sampled_test_preds), 4)

print("*================================*")
print(f" Mean Squared Error: {mse}")
print(f" Root Mean Squared Error: {rmse}")
print(f" Mean Absolute Error: {mae}")
print(f" R-squared: {r2}")
print("*================================*")

"""### Summary of Implementation Results

![image.png](attachment:76a8ba99-24bd-406e-9211-d8bc03697a0a.png)

- Although **ADASYN - XGBoost** has the lowest MAE and MSE on the test set, it performs worse than a simple mean predictor, as seen by its negative R² value.
- Among the models with ADASYN applied, **ADASYN - XGBoost** has the greatest R² and appropriate MSE and MAE scores. After ADASYN was applied, the initial model's negative R² improved to -0.2255, which is still negative but shows a better match than any other ADASYN models.
- Although the R² of the **ADASYN - Decision Tree** is higher than that of the **non-ADASYN Decision Tree**, it is still negative, indicating poor generalisation of the model.
- Both with and without ADASYN, **Random Forest**'s R² value is comparatively lower than **Decision Tree**'s. This could be a sign of overfitting in the Random Forest model that ADASYN helped to mitigate.
- In terms of R² on the test set, the non-ADASYN models typically perform badly; negative values denote poor prediction quality.

It may appear that the **ADASYN - XGBoost** has the lowest test set errors if we disregard the R² values, which are unreliable when they are negative, and simply concentrate on MSE, RMSE, and MAE. Alternatively, the **ADASYN - Logistic Regression** is the best when taking into account the evaluation score balance between the test and train sets.

Since balanced evaluation scores between training and test sets are often chosen since they imply a model that would perform reliably in production, we would base our model selection on the word "balance." Consequently, we would select **ADASYN - Logistic Regression** as the optimal machine learning model for the prediction.

【**Conclusion**】 ***ADASYN - Logistic Regression*** would be suitable for the feature importance.

## Check the feature importance to identify main factors affecting the Target Variable
"""

coeff_df = pd.DataFrame(sampled_lr.coef_.flatten(), X_resampled.columns, columns=['Coefficient'])
print(coeff_df)

"""**Discount_offered** has the largest coefficient value </br>
-> this feature might be most influencial on Reach-on-Time factor.

## Conclusion

### **Discount_offered** is the most influential feature that affects on the Shipping Delivery Reach on Time.
"""