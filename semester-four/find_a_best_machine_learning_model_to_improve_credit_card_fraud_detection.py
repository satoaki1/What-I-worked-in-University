# -*- coding: utf-8 -*-
"""Find a best Machine Learning Model to improve Credit Card Fraud Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r7p-HFzQF6DJTNzhbLJAMMtyk-Ae6YiJ
"""

# Commented out IPython magic to ensure Python compatibility.
# Import necessary libraries

# for Exploratory Data Analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# for Data Preprocessing
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Models to be experimented
from sklearn.linear_model import LogisticRegression  # Logistic Regression
from sklearn.tree import DecisionTreeClassifier  # Decision Tree
from sklearn.neighbors import KNeighborsClassifier  # KNN Classifier

# Evaluation Metrics for evaluations
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, recall_score, precision_score, f1_score

# %matplotlib inline

# To prevent FutureWarning display in output since the warning category is non-critical in experiment
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""## Load the csv file and create a dataframe with the dataset"""

df = pd.read_csv("0354208_Satoakilshihara_Find_a_best_Machine_Learning_Model_to_improve_Credit_Card_Fraud_Detection.ipynb")

"""## EDA

### Initial EDA
"""

df

df.info()

df.describe()

df.shape

df.isna().sum().max()

print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')
print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')

"""### Descriptive EDA with Visualization"""

# Scatter Plot
for column in df:
    plt.figure(figsize = (10, 8))
    plt.hist(df[column])
    plt.title(f"Frequency distributions of values in {column}")
    plt.xlabel(f"{column}")
    plt.ylabel("Frequency")
    plt.show()

# Box Plot
for column in df:
    plt.boxplot(df[column])
    plt.xlabel(f"{column}")
    plt.ylabel("value")
    plt.title(f"Box plot for {column} variable")
    plt.show()

# Scatter Plot
for column in df.drop("Class", axis = 1):
    plt.figure(figsize = (10, 8))
    plt.scatter(df[column], df["Class"])
    plt.title(f"Relationships between Class and {column}")
    plt.xlabel(f"{column}")
    plt.ylabel("Class")
    plt.show()

# Correlation Matrix
plt.figure(figsize=(40, 40))
sns.heatmap(df.corr(), annot = True, cmap = "coolwarm")
plt.show()

"""## PCA Dimensionality Reduction"""

# Standardize the features (important for PCA)
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df.drop(["id", "Class"], axis = 1))

# Initialize PCA
pca = PCA(n_components='mle')

# Fit PCA on the scaled data and transform it
pca_result = pca.fit(df_scaled)

# Elbow Curve plot to find the number of features that eigenvalue is 1
plt.plot(pca_result.explained_variance_)
plt.xlabel("Number of features")
plt.ylabel("Eigenvalues")
plt.title("PCA Eigenvalues")
plt.ylim(0, max(pca_result.explained_variance_))
plt.style.context("seaborn-whitegrid")
plt.axhline(y = 1, color = "r", linestyle = "--")
plt.show()

# Standardize the features
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df.drop(["id", "Class"], axis = 1))

# Initialize PCA (Since the max number of features with eigenvalue 1 is "7", set parameter n_components as 7)
pca = PCA(n_components=7)

# Fit PCA on the scaled data and transform it
pca_result = pca.fit_transform(df_scaled)

# The result is an array of the principal components. To put this back into a DataFrame:
pca_features = pd.DataFrame(pca_result, columns=[f'PC{i+1}' for i in range(pca_result.shape[1])])
pca_target = df["Class"]
pca_dataframe = pd.concat([pca_features, pca_target], axis = 1)

pca_dataframe

"""## Modelling

### Splitting the data into features and target variable
"""

# Features and the Target variable with original, non-reduced data
X = df.drop(["id", "Class"], axis = 1)
y = df["Class"]

# Features and the Target variable with reduced data
X_reduced = pca_dataframe.drop("Class", axis = 1)
y_reduced = pca_dataframe["Class"]

# The Train set and the Test set with original, non-reduced data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# The Train set and the Test set with reduced data
X_reduced_train, X_reduced_test, y_reduced_train, y_reduced_test = train_test_split(X_reduced, y_reduced, test_size = 0.2, random_state = 42)

# Feature scaling to bring all features to the same scale
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""### Base Model

#### Logistic Regression

##### Logistic Regression with original data sets
"""

# Initializing the Logistic Regression model
LR = LogisticRegression(max_iter = 10000)

# Fitting the model with training data
LR.fit(X_train, y_train)

# Making predictions on the test data
lr_train_preds = LR.predict(X_train)
lr_test_preds = LR.predict(X_test)

# Evaluate the Train set
print(classification_report(y_train, lr_train_preds))
lr_cm_train = confusion_matrix(y_train, lr_train_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = lr_cm_train, display_labels = LR.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_train, lr_train_preds), 4)
accuracy = round(accuracy_score(y_train, lr_train_preds), 4)
recall = round(recall_score(y_train, lr_train_preds), 4)
precision = round(precision_score(y_train, lr_train_preds), 4)
f_one = round(f1_score(y_train, lr_train_preds), 4)

# Return evaluation scores
print(f"=========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"=========================")

# Evaluate the Test set
print(classification_report(y_test, lr_test_preds))
lr_cm_test = confusion_matrix(y_test, lr_test_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = lr_cm_test, display_labels = LR.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_test, lr_test_preds), 4)
accuracy = round(accuracy_score(y_test, lr_test_preds), 4)
recall = round(recall_score(y_test, lr_test_preds), 4)
precision = round(precision_score(y_test, lr_test_preds), 4)
f_one = round(f1_score(y_test, lr_test_preds), 4)

# Return evaluation scores
print(f"========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"========================")

"""##### Logistic Regression with reduced data sets"""

# Initializing the Logistic Regression model
LR_reduced = LogisticRegression(max_iter = 10000)

# Fitting the model with training data
LR_reduced.fit(X_reduced_train, y_reduced_train)

# Making predictions on the test data
lr_reduced_train_preds = LR_reduced.predict(X_reduced_train)
lr_reduced_test_preds = LR_reduced.predict(X_reduced_test)

# Evaluate the Train set
print(classification_report(y_reduced_train, lr_reduced_train_preds))
lr_reduced_cm_train = confusion_matrix(y_reduced_train, lr_reduced_train_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = lr_reduced_cm_train, display_labels = LR_reduced.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_reduced_train, lr_reduced_train_preds), 4)
accuracy = round(accuracy_score(y_reduced_train, lr_reduced_train_preds), 4)
recall = round(recall_score(y_reduced_train, lr_reduced_train_preds), 4)
precision = round(precision_score(y_reduced_train, lr_reduced_train_preds), 4)
f_one = round(f1_score(y_reduced_train, lr_reduced_train_preds), 4)

# Return evaluation scores
print(f"========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"========================")

# Evaluate the Test set
print(classification_report(y_reduced_test, lr_reduced_test_preds))
lr_reduced_cm_test = confusion_matrix(y_reduced_test, lr_reduced_test_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = lr_reduced_cm_test, display_labels = LR_reduced.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_reduced_test, lr_reduced_test_preds), 4)
accuracy = round(accuracy_score(y_reduced_test, lr_reduced_test_preds), 4)
recall = round(recall_score(y_reduced_test, lr_reduced_test_preds), 4)
precision = round(precision_score(y_reduced_test, lr_reduced_test_preds), 4)
f_one = round(f1_score(y_reduced_test, lr_reduced_test_preds), 4)

# Return evaluation scores
print(f"========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"========================")

"""##### LR Result

**Logistic Regression with original data sets** was outperformed than the other one with original data sets.

#### Decision Tree

##### Decision Tree with original data sets
"""

# Initializing the Decision Tree model
DT = DecisionTreeClassifier()

# Fitting the model with training data
DT.fit(X_train, y_train)

# Making predictions on the test data
dt_train_preds = DT.predict(X_train)
dt_test_preds = DT.predict(X_test)

# Evaluate the Train set
print(classification_report(y_train, dt_train_preds))
dt_cm_train = confusion_matrix(y_train, dt_train_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = dt_cm_train, display_labels = DT.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_train, dt_train_preds), 4)
accuracy = round(accuracy_score(y_train, dt_train_preds), 4)
recall = round(recall_score(y_train, dt_train_preds), 4)
precision = round(precision_score(y_train, dt_train_preds), 4)
f_one = round(f1_score(y_train, dt_train_preds), 4)

# Return evaluation scores
print(f"=========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"=========================")

# Evaluate the Test set
print(classification_report(y_test, dt_test_preds))
dt_cm_test = confusion_matrix(y_test, dt_test_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = dt_cm_test, display_labels = DT.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_test, dt_test_preds), 4)
accuracy = round(accuracy_score(y_test, dt_test_preds), 4)
recall = round(recall_score(y_test, dt_test_preds), 4)
precision = round(precision_score(y_test, dt_test_preds), 4)
f_one = round(f1_score(y_test, dt_test_preds), 4)

# Return evaluation scores
print(f"========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"========================")

"""##### Decision Tree with reduced data sets"""

# Initializing the Decision Tree model
dt_reduced = DecisionTreeClassifier()

# Fitting the model with training data
dt_reduced.fit(X_reduced_train, y_reduced_train)

# Making predictions on the test data
dt_reduced_train_preds = dt_reduced.predict(X_reduced_train)
dt_reduced_test_preds = dt_reduced.predict(X_reduced_test)

# Evaluate the Train set
print(classification_report(y_reduced_train, dt_reduced_train_preds))
dt_reduced_cm_train = confusion_matrix(y_reduced_train, dt_reduced_train_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = dt_reduced_cm_train, display_labels = dt_reduced.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_reduced_train, dt_reduced_train_preds), 4)
accuracy = round(accuracy_score(y_reduced_train, dt_reduced_train_preds), 4)
recall = round(recall_score(y_reduced_train, dt_reduced_train_preds), 4)
precision = round(precision_score(y_reduced_train, dt_reduced_train_preds), 4)
f_one = round(f1_score(y_reduced_train, dt_reduced_train_preds), 4)

# Return evaluation scores
print(f"========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"========================")

# Evaluate the Test set
print(classification_report(y_reduced_test, dt_reduced_test_preds))
dt_reduced_cm_test = confusion_matrix(y_reduced_test, dt_reduced_test_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = dt_reduced_cm_test, display_labels = dt_reduced.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_reduced_test, dt_reduced_test_preds), 4)
accuracy = round(accuracy_score(y_reduced_test, dt_reduced_test_preds), 4)
recall = round(recall_score(y_reduced_test, dt_reduced_test_preds), 4)
precision = round(precision_score(y_reduced_test, dt_reduced_test_preds), 4)
f_one = round(f1_score(y_reduced_test, dt_reduced_test_preds), 4)

# Return evaluation scores
print(f"========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"========================")

"""##### DT Result

**Decision Tree with original data sets** was outperformed than the other one with PCA-reduced data sets.

#### K-Nearest Neighbors (KNN)

##### KNN with original data sets
"""

# Create an instance of KNeighborsClassifier()
KNC = KNeighborsClassifier()

# Fitting the model with training data
KNC.fit(X_train, y_train)

# Predict the train and test set
knc_train_preds = KNC.predict(X_train)
knc_test_preds = KNC.predict(X_test)

# Evaluate the Train set
print(classification_report(y_train, knc_train_preds))
knc_cm_train = confusion_matrix(y_train, knc_train_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = knc_cm_train, display_labels = KNC.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_train, knc_train_preds), 4)
accuracy = round(accuracy_score(y_train, knc_train_preds), 4)
recall = round(recall_score(y_train, knc_train_preds), 4)
precision = round(precision_score(y_train, knc_train_preds), 4)
f_one = round(f1_score(y_train, knc_train_preds), 4)

# Return evaluation scores
print(f"=========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"=========================")

# Evaluate the Test set
print(classification_report(y_test, knc_test_preds))
knc_cm_test = confusion_matrix(y_test, knc_test_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = knc_cm_test, display_labels = KNC.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_test, knc_test_preds), 4)
accuracy = round(accuracy_score(y_test, knc_test_preds), 4)
recall = round(recall_score(y_test, knc_test_preds), 4)
precision = round(precision_score(y_test, knc_test_preds), 4)
f_one = round(f1_score(y_test, knc_test_preds), 4)

# Return evaluation scores
print(f"=========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"=========================")

"""##### KNN with reduced data sets"""

# Create an instance of KNeighborsClassifier()
KNC_reduced = KNeighborsClassifier()

# Fitting the model with training data
KNC_reduced.fit(X_reduced_train, y_reduced_train)

# Predict the train and test set
knc_reduced_train_preds = KNC_reduced.predict(X_reduced_train)
knc_reduced_test_preds = KNC_reduced.predict(X_reduced_test)

# Evaluate the Train set
print(classification_report(y_reduced_train, knc_reduced_train_preds))
knc_reduced_cm_train = confusion_matrix(y_reduced_train, knc_reduced_train_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = knc_reduced_cm_train, display_labels = KNC_reduced.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_reduced_train, knc_reduced_train_preds), 4)
accuracy = round(accuracy_score(y_reduced_train, knc_reduced_train_preds), 4)
recall = round(recall_score(y_reduced_train, knc_reduced_train_preds), 4)
precision = round(precision_score(y_reduced_train, knc_reduced_train_preds), 4)
f_one = round(f1_score(y_reduced_train, knc_reduced_train_preds), 4)

# Return evaluation scores
print(f"=========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"=========================")

# Evaluate the Test set
print(classification_report(y_reduced_test, knc_reduced_test_preds))
knc_reduced_cm_test = confusion_matrix(y_reduced_test, knc_reduced_test_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = knc_reduced_cm_test, display_labels = KNC_reduced.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_reduced_test, knc_reduced_test_preds), 4)
accuracy = round(accuracy_score(y_reduced_test, knc_reduced_test_preds), 4)
recall = round(recall_score(y_reduced_test, knc_reduced_test_preds), 4)
precision = round(precision_score(y_reduced_test, knc_reduced_test_preds), 4)
f_one = round(f1_score(y_reduced_test, knc_reduced_test_preds), 4)

# Return evaluation scores
print(f"=========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"=========================")

"""##### KNN Result

**K-Nearest Neighbors with original data sets** was outperformed than the other one with original data sets.

### Hyperparameter Tuning

#### Logistic Regression Hyperparameter Tuning
"""

# Define the hyperparameters and their possible values
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}

# Define multiple scoring metrics
scoring_metrics = {
    "ROC-AUC": make_scorer(roc_auc_score),
    'accuracy': make_scorer(accuracy_score),
    "recall": make_scorer(recall_score),
    "precision": make_scorer(precision_score),
    'f1': make_scorer(f1_score, average='weighted')
}

# Instantiate the grid search with the model and the hyperparameters
grid_search = GridSearchCV(estimator=LR, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring=scoring_metrics, refit="ROC-AUC")

# Fit the grid search to the reduced data
grid_search.fit(X_train, y_train)

# Search the best parameters with training data
lr_best_params = grid_search.best_params_
print("Best Hyperparameters:", lr_best_params)

# Define the best hyperparameters
C = lr_best_params["C"]
penalty = lr_best_params["penalty"]
solver = lr_best_params["solver"]

# Re-create the Logistic Regression class instance with the defined hyperparameters
lr_tuned = LogisticRegression(C = C, penalty = penalty, solver = solver)

# Fitting the model with training data
lr_tuned.fit(X_train, y_train)

# Re-predict the train and test set
lr_tuned_train_preds = lr_tuned.predict(X_train)
lr_tuned_test_preds = lr_tuned.predict(X_test)

# Evaluate the Train set
print(classification_report(y_train, lr_tuned_train_preds))
lr_tuned_cm_train = confusion_matrix(y_train, lr_tuned_train_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = lr_tuned_cm_train, display_labels = lr_tuned.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_train, lr_tuned_train_preds), 4)
accuracy = round(accuracy_score(y_train, lr_tuned_train_preds), 4)
recall = round(recall_score(y_train, lr_tuned_train_preds), 4)
precision = round(precision_score(y_train, lr_tuned_train_preds), 4)
f_one = round(f1_score(y_train, lr_tuned_train_preds), 4)

# Return evaluation scores
print(f"========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"========================")

# Evaluate the Test set
print(classification_report(y_test, lr_tuned_test_preds))
lr_tuned_cm_test = confusion_matrix(y_test, lr_tuned_test_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = lr_tuned_cm_test, display_labels = lr_tuned.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_test, lr_tuned_test_preds), 4)
accuracy = round(accuracy_score(y_test, lr_tuned_test_preds), 4)
recall = round(recall_score(y_test, lr_tuned_test_preds), 4)
precision = round(precision_score(y_test, lr_tuned_test_preds), 4)
f_one = round(f1_score(y_test, lr_tuned_test_preds), 4)

# Return evaluation scores
print(f"========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"========================")

"""#### Decision Tree Hyperparameter Tuning"""

# define some candidate parameter values
params_grid = {
    'max_depth': [1, 3, 5, 7, 8, 10, 12, 14, 15, 17, 18, 20],
    'min_samples_split': [8, 10, 12, 18, 20, 16]
}

# Define multiple scoring metrics
scoring_metrics = {
    "ROC-AUC": make_scorer(roc_auc_score),
    'accuracy': make_scorer(accuracy_score),
    "recall": make_scorer(recall_score),
    "precision": make_scorer(precision_score),
    'f1': make_scorer(f1_score, average='weighted')
}

# Instantiate the grid search with the model and the hyperparameters
grid_search = GridSearchCV(estimator=DT, param_grid=params_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring=scoring_metrics, refit = "ROC-AUC")

# Search the best parameters with training data
grid_search.fit(X_train, y_train)

# Search the best parameters with training data
dt_best_params = grid_search.best_params_
print("Best Hyperparameters:", dt_best_params)

# Define the best hyperparameters
max_depth = dt_best_params["max_depth"]
min_samples_split = dt_best_params["min_samples_split"]

# Re-create the Logistic Regression class instance with the defined hyperparameters
dt_tuned = DecisionTreeClassifier(max_depth = max_depth, min_samples_split = min_samples_split)

# Fitting the model with training data
dt_tuned.fit(X_train, y_train)

# Re-predict the train and test set
dt_tuned_train_preds = dt_tuned.predict(X_train)
dt_tuned_test_preds = dt_tuned.predict(X_test)

# Evaluate the Train set
print(classification_report(y_train, dt_tuned_train_preds))
dt_tuned_cm_train = confusion_matrix(y_train, dt_tuned_train_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = dt_tuned_cm_train, display_labels = dt_tuned.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_train, dt_tuned_train_preds), 4)
accuracy = round(accuracy_score(y_train, dt_tuned_train_preds), 4)
recall = round(recall_score(y_train, dt_tuned_train_preds), 4)
precision = round(precision_score(y_train, dt_tuned_train_preds), 4)
f_one = round(f1_score(y_train, dt_tuned_train_preds), 4)

# Return evaluation scores
print(f"========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"========================")

# Evaluate the Test set
print(classification_report(y_test, dt_tuned_test_preds))
dt_tuned_cm_test = confusion_matrix(y_test, dt_tuned_test_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = dt_tuned_cm_test, display_labels = dt_tuned.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_test, dt_tuned_test_preds), 4)
accuracy = round(accuracy_score(y_test, dt_tuned_test_preds), 4)
recall = round(recall_score(y_test, dt_tuned_test_preds), 4)
precision = round(precision_score(y_test, dt_tuned_test_preds), 4)
f_one = round(f1_score(y_test, dt_tuned_test_preds), 4)

# Return evaluation scores
print(f"========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"========================")

"""#### KNN Hyperparameter Tuning"""

# Define the hyperparameters and their possible values
params_grid = {
    'n_neighbors' : [5,7,9,11,13,15],
    'weights' : ['uniform','distance'],
    'metric' : ['minkowski','euclidean','manhattan']
}

# Define multiple scoring metrics
scoring_metrics = {
    "ROC-AUC": make_scorer(roc_auc_score),
    'accuracy': make_scorer(accuracy_score),
    "recall": make_scorer(recall_score),
    "precision": make_scorer(precision_score),
    'f1': make_scorer(f1_score, average='weighted')
}

# Instantiate the grid search with the model and the hyperparameters
grid_search = GridSearchCV(estimator=KNC, param_grid=params_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring=scoring_metrics, refit='ROC-AUC')

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
KNC_best_params = grid_search.best_params_
print("Best Hyperparameters:", KNC_best_params)

# Define the best hyperparameters
n_neighbors = KNC_best_params["n_neighbors"]
weights = KNC_best_params["weights"]
metric = KNC_best_params["metric"]

# Re-create the Logistic Regression class instance with the defined hyperparameters
knc_tuned = KNeighborsClassifier(n_neighbors = n_neighbors, weights = weights, metric = metric)

# Fitting the model with training data
knc_tuned.fit(X_train, y_train)

# Re-predict the train and test set
knc_tuned_train_preds = knc_tuned.predict(X_train)
knc_tuned_test_preds = knc_tuned.predict(X_test)

# Evaluate the Train set
print(classification_report(y_train, knc_tuned_train_preds))
knc_tuned_cm_train = confusion_matrix(y_train, knc_tuned_train_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = knc_tuned_cm_train, display_labels = KNC_reduced.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_train, knc_tuned_train_preds), 4)
accuracy = round(accuracy_score(y_train, knc_tuned_train_preds), 4)
recall = round(recall_score(y_train, knc_tuned_train_preds), 4)
precision = round(precision_score(y_train, knc_tuned_train_preds), 4)
f_one = round(f1_score(y_train, knc_tuned_train_preds), 4)

# Return evaluation scores
print(f"=========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"=========================")

# Evaluate the Test set
print(classification_report(y_test, knc_tuned_test_preds))
knc_tuned_cm_test = confusion_matrix(y_test, knc_tuned_test_preds)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = knc_tuned_cm_test, display_labels = KNC_reduced.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_test, knc_tuned_test_preds), 4)
accuracy = round(accuracy_score(y_test, knc_tuned_test_preds), 4)
recall = round(recall_score(y_test, knc_tuned_test_preds), 4)
precision = round(precision_score(y_test, knc_tuned_test_preds), 4)
f_one = round(f1_score(y_test, knc_tuned_test_preds), 4)

# Return evaluation scores
print(f"=========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"=========================")

"""#### Summary

**fine-tuned K-Nearest Neighbors with original dataset** was outperformed the most.
"""