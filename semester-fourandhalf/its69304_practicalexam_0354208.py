# -*- coding: utf-8 -*-
"""ITS69304_PracticalExam_0354208.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TKmLdHl4gPKSWEc2nPD9Z7RXsALKLW5P
"""

# Import Necessary Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency # module for Chi-Squared Test

# Modules for pre-processing
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder # Encoder to convert object variables into numerical
from sklearn.preprocessing import StandardScaler # for dataset scaling

# Modules of Machine Learning Models
from sklearn.linear_model import LogisticRegression # Logistic Regression model class
from sklearn.ensemble import RandomForestClassifier # Random Forest Classifier class
from sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbors Classifier class

# Modules for Confusion Matrix plot generation
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix

# Modules for evaluation metrics
from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, roc_auc_score

# Module to ignore the non-crucial warning messages on console
import warnings
warnings.filterwarnings("ignore")

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount = True)

"""## Data Collection"""

# Load datasets and create a DataFrame object for each
df_train = pd.read_csv("/content/drive/MyDrive/Taylor's university lesson/Data Analytics and Machine Learning/Mid Term/train.csv")
df_test = pd.read_csv("/content/drive/MyDrive/Taylor's university lesson/Data Analytics and Machine Learning/Mid Term/test.csv")
df_gs = pd.read_csv("/content/drive/MyDrive/Taylor's university lesson/Data Analytics and Machine Learning/Mid Term/gender_submission.csv")

# Since df_test does not have target variable, combine the "Survived" in df_gs
df_test = pd.concat([df_test, df_gs["Survived"]], axis = 1, join = 'inner')

# Print the first 5 observations in df_train
df_train.head()

# Print the first 5 observations in df_test
df_test.head()

"""## Question 1. Exploratory Data Analysis

#### Descriptive Analysis
"""

# Attributes' names, non-null values counts, and data type in df_train
df_train.info()

# Attributes' names, non-null values counts, and data type in df_test
df_test.info()

# Represents the dimension information of df_train
print("The rows and columns of the train data is ", df_train.shape, ".", sep="")

# Represents the dimension information of df_test
print("The rows and columns of the test data is ", df_test.shape, ".", sep="")

# Statistical information of numerical attributes in df_train
df_train.describe()

# Statistical information of categorical attributes in df_train
df_train.describe(include="O")

# Statistical information of numerical attributes in df_test
df_test.describe()

# Statistical information of categorical attributes in df_test
df_test.describe(include="O")

"""#### Analysis with Visualizations


"""

# Determine numerical and categorical features in df_train for Visualization
train_numerical = df_train.drop(columns = ["PassengerId", "Name", "Ticket", "Sex", "Embarked"])
train_categorical = df_train[["Sex", "Embarked"]]

# Determine numerical and categorical features in df_test for Visualization
test_numerical = df_test.drop(columns = ["PassengerId", "Name", "Ticket", "Sex", "Embarked"])
test_categorical = df_test[["Sex", "Embarked"]]

# Histogram for numeric variables in train data set
for col in train_numerical:
    if df_train[col].dtype == int or df_train[col].dtype == float:
        sns.histplot(df_train[col])
        plt.xlabel(f"{col}")
        plt.ylabel(f"Frequencies")
        plt.title(f"The distribution illustration of {col}")
        plt.show()

# Histogram for numeric variables in test data set
for col in test_numerical:
    if df_test[col].dtype == int or df_test[col].dtype == float:
        sns.histplot(df_test[col])
        plt.xlabel(f"{col}")
        plt.ylabel(f"Frequencies")
        plt.title(f"The distribution illustration of {col}")
        plt.show()

# Box plot for numeric variables in train data set
for col in train_numerical:
    if df_train[col].dtype == int or df_train[col].dtype == float:
        sns.boxplot(df_train[col])
        plt.title(f"Boxplot of {col}")
        plt.show()

# Box plot for numeric variables in test data set
for col in test_numerical:
    if df_test[col].dtype == int or df_test[col].dtype == float:
        sns.boxplot(df_test[col])
        plt.title(f"Boxplot of {col}")
        plt.show()

survived_label = 'Survived'
not_survived_label = 'Not Survived'
gender_categories = ['female', 'male']
colors = ['blue', 'red']

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))

# Loop through each gender and plot
for i, gender in enumerate(gender_categories):
    subset = df_train[df_train['Sex'] == gender]

    for survived, color in zip([1, 0], colors):
        sns.histplot(subset[subset['Survived'] == survived]['Age'].dropna(), bins=20, label=f"{survived_label if survived else not_survived_label}", ax=axes[i], color=color, kde=True)

    axes[i].legend()
    axes[i].set_title(f"{gender.capitalize()}")

plt.show()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))

# Loop through each gender and plot
for i, gender in enumerate(gender_categories):
    subset = df_test[df_test['Sex'] == gender]

    for survived, color in zip([1, 0], colors):
        sns.histplot(subset[subset['Survived'] == survived]['Age'].dropna(), bins=20, label=f"{survived_label if survived else not_survived_label}", ax=axes[i], color=color, kde=True)

    axes[i].legend()
    axes[i].set_title(f"{gender.capitalize()}")

plt.show()

# distribution of passengers survived versus non-survived, categorized by their ticket class on the Titanic in df_train
sns.countplot(x = 'Pclass', hue = 'Survived', data = df_train)
plt.title("Passenger Class Distribution - Survived vs Non-Survived");

# distribution of passengers survived versus non-survived, categorized by their ticket class on the Titanic in df_test
sns.countplot(x = 'Pclass', hue = 'Survived', data = df_test)
plt.title("Passenger Class Distribution - Survived vs Non-Survived");

"""## Question 2. Data Pre-Processing

### (a) pre-process the data with tools

#### Data Cleaning

Remove unnecessary columns for the model training.
"""

# Drop unnecessary attributes from both df_train and df_test
df_train = df_train.drop(columns = ["PassengerId", "Name", "Ticket"])
df_test = df_test.drop(columns = ["PassengerId", "Name", "Ticket"])

# Check the number of missing values of each attribute belongs to the dataset
df_train.isna().sum()

# Check the number of missing values of each attribute belongs to the dataset
df_test.isna().sum()

"""Impute N/A in Train Data Set since our EDA would be messed up without data cleaning"""

# Drop the column if the column has more than 50% of missing values in df_train
for col in df_train:
    if df_train[col].isna().sum() >= (df_train.shape[0] / 2):
        df_train = df_train.drop(columns = col)

# Drop the column if the column has more than 50% of missing values in df_test
for col in df_test:
    if df_test[col].isna().sum() >= (df_test.shape[0] / 2):
        df_test = df_test.drop(columns = col)

# Check whether all those critically missing attributes are demolished in train data
df_train.isna().sum()

# Check whether all those critically missing attributes are demolished in test data
df_test.isna().sum()

# If, the attribute's data type is object: fill in N/A with mode
for col in df_train:
    if df_train[col].isna().sum() == 0:
        continue
    else:
        if df_train[col].dtype == object:
            df_train[col].fillna(df_train[col].mode()[0], inplace = True)

# Fill in missing values in "Age" attribute in train data
df_train["Age"].fillna(df_train["Age"].mean(), inplace = True)

# If, the attribute's data type is object: fill in N/A with mode
for col in df_test:
    if df_test[col].isna().sum() == 0:
        continue
    else:
        if df_test[col].dtype == object:
            df_test[col].fillna(df_test[col].mode()[0], inplace = True)

# Fill in missing values in "Age" and "Fare" attributes in test data
df_test["Age"].fillna(df_test["Age"].mean(), inplace = True)
df_test["Fare"].fillna(df_test["Fare"].median(), inplace = True)

# Check whether the entire N/A values are successfully filled in train data
df_train.isna().sum()

# Check whether the entire N/A values are successfully filled in train data
df_test.isna().sum()

"""#### Encoding Categorical Variables"""

# Create an object of One-Hot Encoder and Ordinal Encoder
ohe = OneHotEncoder()
oe = OrdinalEncoder()

## Encode each attribute with its own suitable encoder (Train data)

# One-Hot Encoding ("Sex", "Embarked")
ohe_data = pd.DataFrame(ohe.fit_transform(df_train[["Sex", "Embarked"]]).toarray())
df_train = df_train.join(ohe_data).drop(columns = ["Sex", "Embarked"])

# Ordinal Encoding ("Pclass")
df_train[["Pclass"]] = oe.fit_transform(df_train[["Pclass"]])

## Encode each attribute with its own suitable encoder (Test data)

# One-Hot Encoding ("Sex", "Embarked")
ohe_data = pd.DataFrame(ohe.fit_transform(df_test[["Sex", "Embarked"]]).toarray())
df_test = df_test.join(ohe_data).drop(columns = ["Sex", "Embarked"])

# Ordinal Encoding ("Pclass")
df_test[["Pclass"]] = oe.fit_transform(df_test[["Pclass"]])

# Convert the all column names data type into string (Train data)
df_train.columns = df_train.columns.astype(str)

# Convert the float type features into integer
for col in df_train:
    if df_train[col].dtype == float:
        df_train[col] = df_train[col].astype(int)

# Convert the all column names data type into string (Test data)
df_test.columns = df_test.columns.astype(str)

# Convert the float type features into integer
for col in df_test:
    if df_test[col].dtype == float:
        df_test[col] = df_test[col].astype(int)

# Confirm all existing attributes are entirely fulfilled, as integer type in df_train
df_train.info()

# Confirm all existing attributes are entirely fulfilled, as integer type in df_test
df_test.info()

"""### (b) Create a correlation heatmap after pre-processing"""

# Heatmap illustrates the entire correlation matrix
plt.subplots(figsize = (12, 12))
sns.heatmap(df_train.corr(), annot = True)
plt.show()

"""The `Fare` attribute shows a positive correlation with "Survived," suggesting that passengers who paid higher fares were more likely to survive. This could be because higher fares might correspond to higher classes, which had better access to lifeboats.

The `Pclass` attribute has a negative correlation with "Survived." This indicates that passengers in higher classes (where 1st class is the highest and 3rd class is the lowest) were more likely to survive, which is consistent with historical accounts of the Titanic disaster.

The `0` and `1` attributes, which are the “Sex” attribute components encoded with One-Hot Encoder, would likely be very significant based on historical accounts. This could be because female passengers had a higher survival rate.

The 2 and 4 attributes are also having relatively higher correlations with “Survived”.

Therefore,

- Fare
- Pclass
- 0  
- 1
- 2
- 4

would be deployed to predict the outcome of Survived.

## Question 3. Model Training

### Split the Data, and Scale with StandardScaler
"""

# Determine the independent variables and target variable
X_train = df_train[["Fare", "Pclass", "0", "1", "2", "4"]]
y_train = df_train["Survived"]
X_test = df_test[["Fare", "Pclass", "0", "1", "2", "4"]]
y_test = df_test["Survived"]

# Feature scaling to bring all features to the same scale
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

"""### Logistic Regression"""

# Initializing the Logistic Regression model
LR = LogisticRegression(max_iter = 10000)

# Fitting the model with training data
LR.fit(X_train, y_train)

# Making predictions on the test data
lr_pred = LR.predict(X_test)

"""### Random Forest"""

# Initializing the Logistic Regression model
RFC = RandomForestClassifier()

# Fitting the model with training data
RFC.fit(X_train, y_train)

# Making predictions on the test data
rfc_pred = RFC.predict(X_test)

"""### K-Nearest Neighbors"""

# Initializing the Logistic Regression model
KNC = KNeighborsClassifier()

# Fitting the model with training data
KNC.fit(X_train, y_train)

# Making predictions on the test data
knc_pred = KNC.predict(X_test)

"""## Question 4. Evaluation and Future Perspective

### Logistic Regression Evaluation
"""

# Evaluate the Train set
print(classification_report(y_test, lr_pred))
lr_cm = confusion_matrix(y_test, lr_pred)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = lr_cm, display_labels = LR.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_test, lr_pred), 4)
accuracy = round(accuracy_score(y_test, lr_pred), 4)
recall = round(recall_score(y_test, lr_pred), 4)
precision = round(precision_score(y_test, lr_pred), 4)
f_one = round(f1_score(y_test, lr_pred), 4)

# Return evaluation scores
print(f"=========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"=========================")

"""### Random Forest Evaluation"""

# Evaluate the Train set
print(classification_report(y_test, rfc_pred))
rfc_cm = confusion_matrix(y_test, rfc_pred)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = rfc_cm, display_labels = RFC.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_test, rfc_pred), 4)
accuracy = round(accuracy_score(y_test, rfc_pred), 4)
recall = round(recall_score(y_test, rfc_pred), 4)
precision = round(precision_score(y_test, rfc_pred), 4)
f_one = round(f1_score(y_test, rfc_pred), 4)

# Return evaluation scores
print(f"=========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"=========================")

"""### K-Nearest Neighbors Evaluation"""

# Evaluate the Train set
print(classification_report(y_test, knc_pred))
knc_cm = confusion_matrix(y_test, knc_pred)
sns.set_context("notebook")
ConfusionMatrixDisplay(confusion_matrix = knc_cm, display_labels = KNC.classes_).plot()

# Evaluation Scores
roc_auc = round(roc_auc_score(y_test, knc_pred), 4)
accuracy = round(accuracy_score(y_test, knc_pred), 4)
recall = round(recall_score(y_test, knc_pred), 4)
precision = round(precision_score(y_test, knc_pred), 4)
f_one = round(f1_score(y_test, knc_pred), 4)

# Return evaluation scores
print(f"=========================")
print(f" ROC-AUC Score: {roc_auc}")
print(f" Accuracy Score: {accuracy}")
print(f" Recall Score: {recall}")
print(f" Precision Score: {precision}")
print(f" F1-score: {f_one}")
print(f"=========================")